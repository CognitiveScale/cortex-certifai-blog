{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different fairness measures\n",
    "\n",
    "#### This notebook is a companion to the blog post `Put in title and link`\n",
    "\n",
    "In it we will compare two popular fairness measures, the Disparate Impact Ratio and the Equal Opportunity Difference, to a new measure of fairness called the Burden Index.\n",
    "\n",
    "We will use the IBM AI Fairness 360 toolbox (to install visit https://github.com/IBM/AIF360) to calculate the Disparate Impact Ratio and the Equal Opportunity Difference, and CognitiveScale's Cortex Certifiai toolkit to calculate the Burden Index (to install visit https://www.cognitivescale.com/certifai/ and sign up). \n",
    "\n",
    "We will train a logistic regression model on the Adult Census data for the task of predicting which individuals in the dataset will make more than $50,000 per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from cat_encoder import CatEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "# import Certifai functions\n",
    "from certifai.scanner.builder import (CertifaiScanBuilder, CertifaiPredictorWrapper, CertifaiModel, CertifaiModelMetric,\n",
    "                                      CertifaiDataset, CertifaiGroupingFeature, CertifaiDatasetSource,\n",
    "                                      CertifaiPredictionTask, CertifaiTaskOutcomes, CertifaiOutcomeValue)\n",
    "from certifai.scanner.report_utils import scores, construct_scores_dataframe\n",
    "\n",
    "# import dataset objects and fairness metric functions from AIF 360\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass   education       marital-status  \\\n",
       "0   39          State-gov   Bachelors        Never-married   \n",
       "1   50   Self-emp-not-inc   Bachelors   Married-civ-spouse   \n",
       "2   38            Private     HS-grad             Divorced   \n",
       "3   53            Private        11th   Married-civ-spouse   \n",
       "4   28            Private   Bachelors   Married-civ-spouse   \n",
       "\n",
       "           occupation    race      sex  capital-gain  capital-loss  \\\n",
       "0        Adm-clerical   White     Male          2174             0   \n",
       "1     Exec-managerial   White     Male             0             0   \n",
       "2   Handlers-cleaners   White     Male             0             0   \n",
       "3   Handlers-cleaners   Black     Male             0             0   \n",
       "4      Prof-specialty   Black   Female             0             0   \n",
       "\n",
       "   hours-per-week  native-country  \n",
       "0              40   United-States  \n",
       "1              13   United-States  \n",
       "2              40   United-States  \n",
       "3              40   United-States  \n",
       "4              40            Cuba  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example will use a simple logistic classifier on the Adult Census dataset\n",
    "all_data_file = \"adult.data\"\n",
    "\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education',\n",
    "            'education-num', 'marital-status', 'occupation', 'relationship',\n",
    "            'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "            'native-country', 'income-per-year']\n",
    "\n",
    "label_column='income-per-year'\n",
    "\n",
    "features_to_drop = ['fnlwgt','relationship','education-num'] # we'll drop these features, \n",
    "        # \"fnlwgt\" was assigned after the data was gathered, so we'll drop it\n",
    "        # \"relationship\" is the relationship of the person who answered the questions to the person who the answers were about\n",
    "        # \"education-num\" is somewhat redundant with \"education\" (you could use education instead)\n",
    " \n",
    "df = pd.read_csv(all_data_file, header=None, names = column_names)\n",
    "\n",
    "df = df.drop(features_to_drop, axis=1)\n",
    "df[label_column] = df[label_column].str.contains(\">50K\").astype(int)\n",
    "\n",
    "cat_columns = ['workclass', 'education',\n",
    " 'marital-status', 'occupation',\n",
    " 'native-country','race','sex']\n",
    "\n",
    "num_columns = [f for f in df.columns if (f not in cat_columns) and (f != label_column)]\n",
    "\n",
    "# Separate outcome\n",
    "y = df[label_column]\n",
    "X = df.drop(label_column, axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'Logistic classifier' accuracy is 0.8565945033010901\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "encoder = CatEncoder(cat_columns, X) # this one-hot encodes the categorical features and standardizes the numerical features\n",
    "\n",
    "def build_model(data, name, model_family, test=None):\n",
    "    if test is None:\n",
    "        test = data    \n",
    "    if model_family == 'SVM':\n",
    "        parameters = {'kernel':('linear', 'rbf', 'poly'), 'C':[0.1, .5, 1, 2, 4, 10], 'gamma':['auto']}\n",
    "        m = svm.SVC()\n",
    "    elif model_family == 'logistic':\n",
    "        parameters = {'C': (0.5, 1.0, 2.0), 'solver': ['liblinear'], 'max_iter': [1000]}\n",
    "        m = LogisticRegression(random_state=4)\n",
    "    model = GridSearchCV(m, parameters, cv=3)\n",
    "    model.fit(data[0], data[1])\n",
    "\n",
    "    # Assess on the test data\n",
    "    accuracy = model.score(test[0], test[1].values)\n",
    "    print(f\"Model '{name}' accuracy is {accuracy}\")\n",
    "    return model\n",
    "\n",
    "logistic_model = build_model((encoder(X_train.values), y_train),\n",
    "                        'Logistic classifier',\n",
    "                        'logistic',\n",
    "                        test=(encoder(X_test.values), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's calculate Disparate Impact Ratio and Equal Opportunity Difference using the AIF 360 toolbox. \n",
    "First, we need to set up the data to be injested by an AIF 360 dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep the data to be injested as an AIF 360 dataset object\n",
    "processed_data_all = pd.DataFrame(encoder(X.to_numpy()),columns=encoder.transformed_features)\n",
    "processed_data_all[label_column] = y\n",
    "\n",
    "# put the data in a StandardDataset object and identify the protected attributes (here that is the sex of the individual)\n",
    "data_aif_all = StandardDataset(processed_data_all,label_name=label_column, favorable_classes=[1],\n",
    "                 protected_attribute_names=['sex_ Female'],\n",
    "                 privileged_classes=[[1]])\n",
    "\n",
    "# define the privileged and unprivileged groups, which will be used to calculate the metrics\n",
    "privileged_groups = [{'sex_ Female': 0}]\n",
    "unprivileged_groups = [{'sex_ Female': 1}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we store the predicted labels and associated probabilities in the dataset object and use the object initialize a ClassificationMetric object, which we can then use to calculate a variety of fairness metrics (60+ of them according to the documentation!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact Ratio:  0.2269\n",
      "Privileged TPR: 0.6268\n",
      "Unprivileged TPR: 0.436\n",
      "Equal Opportunity Difference (Unprivileged TPR - Privileged TPR)): -0.1909\n"
     ]
    }
   ],
   "source": [
    "pred_all = data_aif_all.copy(deepcopy=True)\n",
    "pred_all.labels = logistic_model.predict(encoder(X.to_numpy())).reshape(-1,1)\n",
    "pred_all.scores = logistic_model.predict_proba(encoder(X.to_numpy()))[:,0]\n",
    "class_metric_all = ClassificationMetric(data_aif_all, pred_all,\n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "\n",
    "dis_imp_ratio = class_metric_all.disparate_impact()\n",
    "eq_opp_diff = class_metric_all.equal_opportunity_difference()\n",
    "tpr_privileged = class_metric_all.true_positive_rate(privileged=True)\n",
    "tpr_unprivileged = class_metric_all.true_positive_rate(privileged=False)\n",
    "\n",
    "print('Disparate Impact Ratio: ', np.round(dis_imp_ratio,4))\n",
    "\n",
    "print(\"Privileged TPR:\", np.round(tpr_privileged,4))\n",
    "print(\"Unprivileged TPR:\", np.round(tpr_unprivileged,4))\n",
    "print('Equal Opportunity Difference (Unprivileged TPR - Privileged TPR)):', np.round(eq_opp_diff,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Disparate Impact Ratio of less than .8 is generally considered to be unfair (but this is somewhat arbitrary), so this model would be considered unfair by that measure. However, Disparate Impact does not take into account the ground truth of the data, which can be problematic.\n",
    "\n",
    "The Equal Opportunity Difference should be 0 to be considered completely fair. A negative value of the Equal Opportunity Difference means the model is correctly predicting the privileged class who received favored outcome more often than it is correctly predicting the unprivileged class. However, there isn't a hard threshold for when the difference is unfair. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we'll use the Cortex Certifai toolkit to calculate the Burden Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model up for use by Certifai as a local model\n",
    "model_proxy = CertifaiPredictorWrapper(logistic_model, encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scan with model_use_case_id: 'test_user_case' and scan_id: 'bf110bb6e020'\n",
      "[--------------------] 2020-06-04 15:30:30.683216 - 0 of 1 reports (0.0% complete) - Running fairness evaluation for model: full\n",
      "[####################] 2020-06-04 15:39:14.879823 - 1 of 1 reports (100.0% complete) - Completed all evaluations\n"
     ]
    }
   ],
   "source": [
    "# First define the possible prediction outcomes\n",
    "task = CertifaiPredictionTask(CertifaiTaskOutcomes.classification(\n",
    "    [\n",
    "        CertifaiOutcomeValue(1, name='earned more than 50k per year', favorable=True),\n",
    "        CertifaiOutcomeValue(0, name='earned less than 50k per year')\n",
    "    ]),\n",
    "    prediction_description='Did person earn more than 50k per year')\n",
    "\n",
    "scan = CertifaiScanBuilder.create('test_user_case',\n",
    "                                  prediction_task=task)\n",
    "\n",
    "# Add our local model\n",
    "first_model = CertifaiModel('full',\n",
    "                            local_predictor=model_proxy)\n",
    "scan.add_model(first_model)\n",
    "\n",
    "# Add the eval dataset\n",
    "eval_dataset = CertifaiDataset('evaluation',\n",
    "                               CertifaiDatasetSource.dataframe(df))\n",
    "scan.add_dataset(eval_dataset)\n",
    "\n",
    "# Setup an evaluation for fairness on the above dataset using the model\n",
    "# We'll look at disparity in the features we hid from the model\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('sex'))\n",
    "# scan.add_fairness_grouping_feature(CertifaiGroupingFeature('race')) # you can take a look at the burden for race too\n",
    "scan.add_evaluation_type('fairness')\n",
    "scan.evaluation_dataset_id = 'evaluation'\n",
    "\n",
    "# We can calculate other measures of fairness as well\n",
    "scan.add_fairness_metric('demographic parity')\n",
    "scan.add_fairness_metric('equal opportunity')\n",
    "\n",
    "# Because the dataset contains a ground truth outcome column which the model does not\n",
    "# expect to receive as input we need to state that in the dataset schema (since it cannot\n",
    "# be inferred from the CSV)\n",
    "scan.dataset_schema.outcome_feature_name = label_column\n",
    "\n",
    "# Run the scan.\n",
    "# By default this will write the results into individual report files (one per model and evaluation\n",
    "# type) in the 'reports' directory relative to the Jupyter root.  This may be disabled by specifying\n",
    "# `write_reports=False` as below\n",
    "# The result is a dictionary of dictionaries of reports.  The top level dict key is the evaluation type\n",
    "# and the second level key is model id.\n",
    "# Reports saved as JSON (which `write_reports=True` will do) may be visualized in the console app\n",
    "result = scan.run(write_reports=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at the Burden Index score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>type</th>\n",
       "      <th>overall fairness</th>\n",
       "      <th>Feature (sex)</th>\n",
       "      <th>Group details ( Female)</th>\n",
       "      <th>Group details ( Male)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>full (burden)</th>\n",
       "      <td>full</td>\n",
       "      <td>burden</td>\n",
       "      <td>74.917046</td>\n",
       "      <td>74.917046</td>\n",
       "      <td>0.219938</td>\n",
       "      <td>0.131663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full (demographic parity)</th>\n",
       "      <td>full</td>\n",
       "      <td>demographic parity</td>\n",
       "      <td>80.186787</td>\n",
       "      <td>80.186787</td>\n",
       "      <td>0.068924</td>\n",
       "      <td>0.253372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full (equal opportunity)</th>\n",
       "      <td>full</td>\n",
       "      <td>equal opportunity</td>\n",
       "      <td>67.537411</td>\n",
       "      <td>67.537411</td>\n",
       "      <td>0.408696</td>\n",
       "      <td>0.609375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          context                type  overall fairness  \\\n",
       "full (burden)                full              burden         74.917046   \n",
       "full (demographic parity)    full  demographic parity         80.186787   \n",
       "full (equal opportunity)     full   equal opportunity         67.537411   \n",
       "\n",
       "                           Feature (sex)  Group details ( Female)  \\\n",
       "full (burden)                  74.917046                 0.219938   \n",
       "full (demographic parity)      80.186787                 0.068924   \n",
       "full (equal opportunity)       67.537411                 0.408696   \n",
       "\n",
       "                           Group details ( Male)  \n",
       "full (burden)                           0.131663  \n",
       "full (demographic parity)               0.253372  \n",
       "full (equal opportunity)                0.609375  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_df = construct_scores_dataframe(scores('fairness', result), include_confidence=False)\n",
    "display(score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Burden_Index = score_df.loc[score_df.type == 'burden','overall fairness'].values[0]\n",
    "DI_unprivileged = score_df.loc[score_df.type == 'demographic parity','Group details ( Female)'].values[0]\n",
    "DI_privileged = score_df.loc[score_df.type == 'demographic parity','Group details ( Male)'].values[0]\n",
    "DI_ratio = DI_unprivileged / DI_privileged\n",
    "TPR_unprivileged = score_df.loc[score_df.type == 'equal opportunity','Group details ( Female)'].values[0]\n",
    "TPR_privileged = score_df.loc[score_df.type == 'equal opportunity','Group details ( Male)'].values[0]\n",
    "EO_difference = TPR_unprivileged - TPR_privileged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `overall fairness` is the Burden Index, and `Group details <<attribute value>>` is the average distance (by group within a protected attribute) between the original observations and their counterfactuals that lie in the favorable class (Here, if someone already receives the favorable outcome, then the distance between their original feature values and the counterfactual that lies in the favorable class is trivially 0. Operationally, this means we add a 1 to the denominator for each person in that group who received the favorable prediction). \n",
    "\n",
    "The Burden Index, which takes on values between 0 and 100, is a Gini-like index that measures the disparity between the `Group details` of the different groups. A low value means that the distance between the observations and their counterfactuals for at least one group is much higher than the other groups. This means it's much harder for them to change their features to gain the favorable prediction. \n",
    "\n",
    "Here the Burden Index is 74.917. Like the Equal Opportunity Difference, there isn't a hard threshold where models with a Burden Index below that value would be considered unfair,  but model developers can use it to compare models without needing to know the ground truth. This is useful in a production settings where ground truth does not exist, and unlike Disparate Impact, the Burden Index takes in to account some notion of \"worthiness\" given the model. \n",
    "Let's take a look at all of the measures together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burden Index:  74.917\n",
      "Disparate Impact Ratio:  0.272\n",
      "Privileged TPR: 0.6094\n",
      "Unprivileged TPR: 0.4087\n",
      "Equal Opportunity Difference (Unprivileged TPR - Privileged TPR)): -0.2007\n",
      "Adult , 74.917 , 0.272 , 0.6094 , 0.4087 , -0.2007\n"
     ]
    }
   ],
   "source": [
    "print('Burden Index: ', np.round(Burden_Index,4))\n",
    "print('Disparate Impact Ratio: ', np.round(DI_ratio,4))\n",
    "\n",
    "print(\"Privileged TPR:\", np.round(TPR_privileged,4))\n",
    "print(\"Unprivileged TPR:\", np.round(TPR_unprivileged,4))\n",
    "print('Equal Opportunity Difference (Unprivileged TPR - Privileged TPR)):', np.round(EO_difference,4))\n",
    "print('Adult',',',np.round(Burden_Index,4),',', np.round(DI_ratio,4),',', np.round(TPR_privileged,4), ',',np.round(TPR_unprivileged,4),',', np.round(EO_difference,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burden Index:  74.917\n",
    "# Disparate Impact Ratio:  0.272\n",
    "# Privileged TPR: 0.6094\n",
    "# Unprivileged TPR: 0.4087\n",
    "# Equal Opportunity Difference (Unprivileged TPR - Privileged TPR)): -0.2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While none of the measures give us a definitive idea of whether or not the model is fair (because it depends on your definition of fairness), they do all say that the model is favoring the privileged group over the unprivileged group. Looking at this, I would say we as model developers should figure out how to do a better job at predicting which women will make more than $50k per year (and we as humans should figure out how to dismantle and reconfigure power structures so all groups have a fairer shot of achieving the outcomes they want).\n",
    "\n",
    "So, which metric should we focus on? Typically there is no right answer: they all tell us something about our model's fairness. Disparate Impact operationalizes the idea of fairness that all groups should be equally likely to have a favorable prediction of the model, but on an individual level, it can be unfair because it does not mean that a \"better\" individual will get the favorable prediction from the model. Equal Opportunity addresses this issue by looking at the people who are \"good enough\" in the eyes of the world (defined as those people who had the positive outcome in the ground truth) and desiring that those people be equally likely to receive the favorable prediction from the model. However, Equal Opportunity has no concept of how hard it would be for certain groups to change their prediction from unfavorable to favorable, which is where the Burden Index comes in. A model could have an Equal Opportunity Difference of 0 and yet 1) it could be nearly impossible for one group to change their features enough to gain the favorable prediction, and/or 2) very few people in one group in the ground truth have the favorable outcome but the model could be predicting on this small group correctly, which doesn't seem great either. The Burden Index will illuminate both of these points and give a more comprehensive view of the fairness of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
