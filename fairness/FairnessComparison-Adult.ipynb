{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different fairness measures\n",
    "\n",
    "### In this notebook, we will compare two popular fairness measures, disparate impact ratio and the equal opportunity difference to a new measure of fairness called the Burden Index\n",
    "\n",
    "We will use the IBM AI Fairness 360 toolbox to calculate the disparate impact ratio and the equal opportunity difference, and CognitiveScale's Cortex Certifiai toolkit to calculate the Burden Index. \n",
    "\n",
    "We will train a logistic regression model on the Adult Census data for the task of predicting which individual's in the dataset will make more than $50,000 per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from cat_encoder import CatEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# import Certifai functions\n",
    "from certifai.scanner.builder import (CertifaiScanBuilder, CertifaiPredictorWrapper, CertifaiModel, CertifaiModelMetric,\n",
    "                                      CertifaiDataset, CertifaiGroupingFeature, CertifaiDatasetSource,\n",
    "                                      CertifaiPredictionTask, CertifaiTaskOutcomes, CertifaiOutcomeValue)\n",
    "from certifai.scanner.report_utils import scores, construct_scores_dataframe\n",
    "\n",
    "# import dataset objects and fairness metric functions from AIF 360\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass   education       marital-status  \\\n",
       "0   39          State-gov   Bachelors        Never-married   \n",
       "1   50   Self-emp-not-inc   Bachelors   Married-civ-spouse   \n",
       "2   38            Private     HS-grad             Divorced   \n",
       "3   53            Private        11th   Married-civ-spouse   \n",
       "4   28            Private   Bachelors   Married-civ-spouse   \n",
       "\n",
       "           occupation    race      sex  capital-gain  capital-loss  \\\n",
       "0        Adm-clerical   White     Male          2174             0   \n",
       "1     Exec-managerial   White     Male             0             0   \n",
       "2   Handlers-cleaners   White     Male             0             0   \n",
       "3   Handlers-cleaners   Black     Male             0             0   \n",
       "4      Prof-specialty   Black   Female             0             0   \n",
       "\n",
       "   hours-per-week  native-country  \n",
       "0              40   United-States  \n",
       "1              13   United-States  \n",
       "2              40   United-States  \n",
       "3              40   United-States  \n",
       "4              40            Cuba  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example will use a simple logistic classifier on the Adult Census dataset\n",
    "all_data_file = \"adult.data\"\n",
    "\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education',\n",
    "            'education-num', 'marital-status', 'occupation', 'relationship',\n",
    "            'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "            'native-country', 'income-per-year']\n",
    "\n",
    "label_column='income-per-year'\n",
    "\n",
    "features_to_drop = ['fnlwgt','relationship','education-num']\n",
    " \n",
    "df = pd.read_csv(all_data_file, header=None, names = column_names)\n",
    "\n",
    "df = df.drop(features_to_drop, axis=1)\n",
    "df[label_column] = df[label_column].str.contains(\">50K\").astype(int)\n",
    "\n",
    "cat_columns = ['workclass', 'education',\n",
    " 'marital-status', 'occupation',\n",
    " 'native-country','race','sex']\n",
    "\n",
    "num_columns = [f for f in df.columns if (f not in cat_columns) and (f != label_column)]\n",
    "\n",
    "# Separate outcome\n",
    "y = df[label_column]\n",
    "X = df.drop(label_column, axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'Logistic classifier' accuracy is 0.8565945033010901\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "encoder = CatEncoder(cat_columns, X)\n",
    "\n",
    "def build_model(data, name, model_family, test=None):\n",
    "    if test is None:\n",
    "        test = data    \n",
    "    if model_family == 'SVM':\n",
    "        parameters = {'kernel':('linear', 'rbf', 'poly'), 'C':[0.1, .5, 1, 2, 4, 10], 'gamma':['auto']}\n",
    "        m = svm.SVC()\n",
    "    elif model_family == 'logistic':\n",
    "        parameters = {'C': (0.5, 1.0, 2.0), 'solver': ['liblinear'], 'max_iter': [1000]}\n",
    "        m = LogisticRegression(random_state=4)\n",
    "    model = GridSearchCV(m, parameters, cv=3)\n",
    "    model.fit(data[0], data[1])\n",
    "\n",
    "    # Assess on the test data\n",
    "    accuracy = model.score(test[0], test[1].values)\n",
    "    print(f\"Model '{name}' accuracy is {accuracy}\")\n",
    "    return model\n",
    "\n",
    "logistic_model = build_model((encoder(X_train.values), y_train),\n",
    "                        'Logistic classifier',\n",
    "                        'logistic',\n",
    "                        test=(encoder(X_test.values), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's calculate Disparate Impact Ratio and Equal Opportunity Difference using the AIF 360 toolbox. \n",
    "First, we need to set up the data to be injested by an AIF 360 dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep the data to be injested as an AIF 360 dataset object\n",
    "processed_data_all = pd.DataFrame(encoder(X.to_numpy()),columns=encoder.transformed_features)\n",
    "processed_data_all[label_column] = y\n",
    "\n",
    "# put the data in a StandardDataset object and identify the protected attributes (here that is the sex of the individual)\n",
    "data_aif_all = StandardDataset(processed_data_all,label_name=label_column, favorable_classes=[1],\n",
    "                 protected_attribute_names=['sex_ Female'],\n",
    "                 privileged_classes=[[1]])\n",
    "\n",
    "# define the privileged and unprivileged groups, which will be used to calculate the metrics\n",
    "privileged_groups = [{'sex_ Female': 0}]\n",
    "unprivileged_groups = [{'sex_ Female': 1}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we store the predicted labels and associated probabilities in the dataset object and use the object initialize a ClassificationMetric object, which we can then use to calculate a variety of fairness metrics (60+ of them according to the documentation!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact Ratio:  0.2269\n",
      "Privileged TPR: 0.6268\n",
      "Unprivileged TPR: 0.436\n",
      "Equal Opportunity Difference (Unprivileged TPR - Privileged TPR)): -0.1909\n"
     ]
    }
   ],
   "source": [
    "pred_all = data_aif_all.copy(deepcopy=True)\n",
    "pred_all.labels = logistic_model.predict(encoder(X.to_numpy())).reshape(-1,1)\n",
    "pred_all.scores = logistic_model.predict_proba(encoder(X.to_numpy()))[:,0]\n",
    "class_metric_all = ClassificationMetric(data_aif_all, pred_all,\n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "\n",
    "dis_imp_ratio = class_metric_all.disparate_impact()\n",
    "eq_opp_diff = class_metric_all.equal_opportunity_difference()\n",
    "tpr_privileged = class_metric_all.true_positive_rate(privileged=True)\n",
    "tpr_unprivileged = class_metric_all.true_positive_rate(privileged=False)\n",
    "\n",
    "print('Disparate Impact Ratio: ', np.round(dis_imp_ratio,4))\n",
    "\n",
    "print(\"Privileged TPR:\", np.round(tpr_privileged,4))\n",
    "print(\"Unprivileged TPR:\", np.round(tpr_unprivileged,4))\n",
    "print('Equal Opportunity Difference (Unprivileged TPR - Privileged TPR)):', np.round(eq_opp_diff,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Disparate Impact Ratio of less than .8 is generally considered to be unfair (but this is somewhat arbitrary), so this model would be considered unfair by that measure. However, Disparate Impact does not take into account the ground truth of the data, which can be problematic.\n",
    "\n",
    "The Equal Opportunity Difference should be 0 to be considered completely fair. A negative value of the Equal Opportunity Difference means the model is correctly predicting the privileged class who received favored outcome more often than it is correctly predicting the unprivileged class. However, there isn't a hard threshold for when the difference is unfair. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we'll use the Cortex Certifai toolkit to calculate the Burden Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model up for use by Certifai as a local model\n",
    "model_proxy = CertifaiPredictorWrapper(logistic_model, encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scan with model_use_case_id: 'test_user_case' and scan_id: '4a7ee70ee300'\n",
      "[--------------------] 2020-06-02 11:46:42.028865 - 0 of 1 reports (0.0% complete) - Running fairness evaluation for model: full\n",
      "[####################] 2020-06-02 11:51:15.255259 - 1 of 1 reports (100.0% complete) - Completed all evaluations\n"
     ]
    }
   ],
   "source": [
    "# First define the possible prediction outcomes\n",
    "task = CertifaiPredictionTask(CertifaiTaskOutcomes.classification(\n",
    "    [\n",
    "        CertifaiOutcomeValue(1, name='earned more than 50k per year', favorable=True),\n",
    "        CertifaiOutcomeValue(0, name='earned less than 50k per year')\n",
    "    ]),\n",
    "    prediction_description='Did person earn more than 50k per year')\n",
    "\n",
    "scan = CertifaiScanBuilder.create('test_user_case',\n",
    "                                  prediction_task=task)\n",
    "\n",
    "# Add our local model\n",
    "first_model = CertifaiModel('full',\n",
    "                            local_predictor=model_proxy)\n",
    "scan.add_model(first_model)\n",
    "\n",
    "# Add the eval dataset\n",
    "eval_dataset = CertifaiDataset('evaluation',\n",
    "                               CertifaiDatasetSource.dataframe(df))\n",
    "scan.add_dataset(eval_dataset)\n",
    "\n",
    "# Setup an evaluation for fairness on the above dataset using the model\n",
    "# We'll look at disparity in the features we hid from the model\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('sex'))\n",
    "# scan.add_fairness_grouping_feature(CertifaiGroupingFeature('race')) # you can take a look at the burden for race too\n",
    "scan.add_evaluation_type('fairness')\n",
    "scan.evaluation_dataset_id = 'evaluation'\n",
    "\n",
    "# Because the dataset contains a ground truth outcome column which the model does not\n",
    "# expect to receive as input we need to state that in the dataset schema (since it cannot\n",
    "# be inferred from the CSV)\n",
    "scan.dataset_schema.outcome_feature_name = label_column\n",
    "\n",
    "# Run the scan.\n",
    "# By default this will write the results into individual report files (one per model and evaluation\n",
    "# type) in the 'reports' directory relative to the Jupyter root.  This may be disabled by specifying\n",
    "# `write_reports=False` as below\n",
    "# The result is a dictionary of dictionaries of reports.  The top level dict key is the evaluation type\n",
    "# and the second level key is model id.\n",
    "# Reports saved as JSON (which `write_reports=True` will do) may be visualized in the console app\n",
    "result = scan.run(write_reports=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at the Burden Index score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>type</th>\n",
       "      <th>overall fairness</th>\n",
       "      <th>Feature (sex)</th>\n",
       "      <th>Group details ( Female)</th>\n",
       "      <th>Group details ( Male)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>full (burden)</th>\n",
       "      <td>full</td>\n",
       "      <td>burden</td>\n",
       "      <td>74.917046</td>\n",
       "      <td>74.917046</td>\n",
       "      <td>0.219938</td>\n",
       "      <td>0.131663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              context    type  overall fairness  Feature (sex)  \\\n",
       "full (burden)    full  burden         74.917046      74.917046   \n",
       "\n",
       "               Group details ( Female)  Group details ( Male)  \n",
       "full (burden)                 0.219938               0.131663  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_df = construct_scores_dataframe(scores('fairness', result), include_confidence=False)\n",
    "display(score_df)\n",
    "burden_index = score_df['overall fairness'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `overall fairness` is the Burden Index, and `Group details <<attribute value>>` is the average distance (by group within a protected attribute) between the original observations and their counterfactuals that lie in the favorable class (Here, if someone already receives the favorable outcome, then the distance between their original feature values and the counterfactual that lies in the favorable class is trivially 0. Operationally, this means we add a 1 to the denominator for each person in that group who received the favorable prediction). \n",
    "\n",
    "The Burden Index, which takes on values between 0 and 100, is a Gini-like index that measures the disparity between the `group details` of the different groups. A low value means that the distance between the observations and their counterfactuals for at least one group is much higher than the other groups. This means it's much harder for them to change their features to gain the favorable prediction. \n",
    "\n",
    "Here the Burden Index is 74.917. Like the Equal Opportunity Difference, there isn't a hard threshold where models with a Burden Index below that value would be considered unfair,  but model developers can use it to compare models without needing to know the ground truth. This is useful in a production settings where ground truth does not exist, and unlike Disparate Impact, the Burden Index takes in to account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burden Index:  74.91704564404755\n",
      "Disparate Impact Ratio:  0.2269\n",
      "Privileged TPR: 0.6268\n",
      "Unprivileged TPR: 0.436\n",
      "Equal Opportunity Difference (Unprivileged TPR - Privileged TPR)): -0.1909\n"
     ]
    }
   ],
   "source": [
    "print('Burden Index: ', np.round(burden_index)\n",
    "print('Disparate Impact Ratio: ', np.round(dis_imp_ratio,4))\n",
    "\n",
    "print(\"Privileged TPR:\", np.round(tpr_privileged,4))\n",
    "print(\"Unprivileged TPR:\", np.round(tpr_unprivileged,4))\n",
    "print('Equal Opportunity Difference (Unprivileged TPR - Privileged TPR)):', np.round(eq_opp_diff,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
