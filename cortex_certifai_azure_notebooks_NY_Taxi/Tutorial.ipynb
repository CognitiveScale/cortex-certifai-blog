{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Build Trusted ML models with Certifai on Azure Notebooks\n",
    "\n",
    "This tutorial picks up after part one of the Azure regression tutorial. In part one you prepared the NYC taxi data for regression modeling. The referenced parts 1 & 2 can be found under My projects on the [Azure Notebooks Portal](https://notebooks.azure.com/)\n",
    "\n",
    "In this tutorial, you learn:\n",
    "\n",
    "> * How to set up a Certifai scan from scratch\n",
    "> * How to run Explainability, Fairness and Robustness scans\n",
    "> * Explore their results\n",
    "> * Log Certifai results to the Azure portal\n",
    "\n",
    "If you don't have an Azure subscription, create a [free account](https://aka.ms/AMLfree) before you begin. \n",
    "\n",
    "If you don't have the Certifai Toolkit, get it now from our [Certifai page](https://www.cognitivescale.com/download-certifai/)\n",
    "\n",
    "## Usage/Preparation Steps\n",
    "There are two ways to enjoy this tutorial\n",
    "1. Running on Azure Notebooks: By logging to the [Azure Notebooks Portal](https://notebooks.azure.com/)\n",
    "2. Running locally\n",
    "\n",
    "\n",
    "### 1. Running on Azure Notebook portal\n",
    "1. In the [Azure Notebooks Portal](https://notebooks.azure.com/) go to My Projects. Use the \"Clone Repo\" button on the top right. Clone this project's [repo](https://github.com/mdungarov-cs/cortex_certifai_azure_notebooks_ny_taxi.git)\n",
    "2. Download the toolkit from [Certifai page](https://www.cognitivescale.com/download-certifai/) and unzip\n",
    "3. Upload the cat_encoder.py from certifai_toolkit/examples/notebooks\n",
    "4. Upload the scanner, engine and from certifai_toolkit/packages folder\n",
    "5. Upload the requirements.txt file from the certifai_toolkit folder\n",
    "6. Use terminal or your notebook to `pip install` the files from steps 3&4 (remember to `pip install -r` the requirements file)  \n",
    "NB: if using hosted terminal to install dependencies: ensure it is in the right environment and running the right python version).  \n",
    "NB: If using the notebook to install dependencies, you might need to restart the notebook after installations\n",
    "\n",
    "You are all set!\n",
    "\n",
    "### 2. Running locally\n",
    "\n",
    "1. Ensure you have a Python 3.x notebook server with the following installed:\n",
    "2. Install Azure SDK dependencies `pip install --upgrade azureml-sdk[notebooks,automl,widgets]`\n",
    "3. Follow the instructions to install the Certifai toolkit and dependencies from the [documentation page](https://cognitivescale.github.io/cortex-certifai/docs/toolkit/setup/install-certifai-cli-lib)\n",
    "\n",
    "You are all set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Contents\n",
    "\n",
    "1. Data prep\n",
    "2. Training an AutoML model\n",
    "3. Model Selection for Certifai scan\n",
    "4. Certifai Scan Setup\n",
    "5. Review of Results and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep\n",
    "\n",
    "This part follows closely part 2 of the tutorial mentioned before, with minor differences to data prep needed for Certifai to run properly.\n",
    "\n",
    "We start by loading data from part 1, and selecting relevant columns, storing results as csv - Certifai will directly consume the data as a csv file. For some of the runs we can also do with a smaller dataset, hence we also prepare a `_sample` dataset with only 500 rows to shorten the time needed to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.dataprep as dprep\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"dflows.dprep\")\n",
    "dflow_prepared = dprep.Dataflow.open(file_path)\n",
    "\n",
    "dflow_reduced = dflow_prepared.keep_columns(['pickup_weekday','pickup_hour', 'distance',\n",
    "                                             'passengers', 'vendor', 'cost'])\n",
    "\n",
    "df=dflow_reduced.to_pandas_dataframe()\n",
    "\n",
    "# NOTE: DATASET CANNOT HAVE AN INDEX COLUMN FOR CERTIFAI\n",
    "df.to_csv('all_data_NY_Taxi.csv',index=False)\n",
    "df.sample(500,random_state=0).to_csv('all_data_NY_Taxi_sample.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we split the data into test and train but also prepare the CatEncoder by specifying the correct columns as categoricals. This enables multi-processing in the context of a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "from cat_encoder import CatEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_path = '.'\n",
    "all_data_file = f\"{base_path}/all_data_NY_taxi.csv\"\n",
    "sample_data_file = f\"{base_path}/all_data_NY_taxi_sample.csv\"\n",
    "df = pd.read_csv(all_data_file)\n",
    "\n",
    "label_column = 'cost'\n",
    "\n",
    "# Separate outcome\n",
    "y = df[label_column]\n",
    "X = df.drop(label_column, axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "cat_columns = [\n",
    "    'vendor',\n",
    "    'pickup_weekday',\n",
    "    'passengers'\n",
    "    ]\n",
    "# Note - to support python multi-processing in the context of a notebook the encoder MUST\n",
    "# be in a separate file, which is why `CatEncoder` is defined outside of this notebook\n",
    "encoder = CatEncoder(cat_columns, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training AutoML\n",
    "\n",
    "The below simply runs Azure AutoML in the same fashion as in the Azure regression Tutorial after the minor modifications to the data and model encoder we had to do.\n",
    "\n",
    "We will:\n",
    "- log in to our Azure Workspace\n",
    "- Set up and run AutoML\n",
    "- Review results and select models for further review using Certifai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Workspace Login\n",
    "\n",
    "- Type in your workspace credentials\n",
    "- use those to creste a config file\n",
    "- build workspace from config file\n",
    "\n",
    "Please note that you need to populate the appropriate credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"<USER-SUBSCRIPTION-ID>\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"<USER-RESOURCE-GROUP>\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"<USER-WORKSPACE-NAME>\")\n",
    "workspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"<USER-WORKSPACE-REGION>\")  # eg. - 'useast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "except:\n",
    "    # Assuming you have set up a fresh resource group, and workspace, in the Azure console then deploy with the\n",
    "    # following\n",
    "    ws = Workspace.create(name=workspace_name,\n",
    "                   subscription_id=subscription_id,\n",
    "                   resource_group=resource_group,\n",
    "                   create_resource_group=False,\n",
    "                   location=workspace_region\n",
    "                   )\n",
    "    # write the details of the workspace to a configuration file to the notebook library\n",
    "    ws.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "experiment = Experiment(ws, \"taxi-experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto ML\n",
    "\n",
    "Setup and run of Auto ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  logging\n",
    "\n",
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\": 2,\n",
    "    \"iterations\": 20,\n",
    "    \"primary_metric\": 'spearman_correlation',\n",
    "    \"featurization\": 'auto',\n",
    "    \"verbosity\": logging.INFO,\n",
    "    \"n_cross_validations\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "automl_config = AutoMLConfig(task='regression',\n",
    "                             debug_log='automated_ml_errors.log',\n",
    "                             X=encoder(np.array(X_train)),\n",
    "                             y=y_train.values.flatten(),\n",
    "                             **automl_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from azureml.core.experiment import Experiment\n",
    "experiment = Experiment(ws, \"taxi-experiment\")\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(local_run).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection for Certifai Scan\n",
    "\n",
    "After training a range of models and can now select a pair of the best performing ones to evaluate with Certifai. Note that you can select any number of models to evaluate, however, we only pick two here for illustration.\n",
    "\n",
    "From the above we see that the best performing model is the Voting Ensemble. Followed closely by a Stack Ensemble model. We will consider the Voting Ensemble to be our \"Champion\" model\n",
    "\n",
    "For our challenger, we will use a the best performing Random Tree Classifier (Model 12, Extreme Random Trees). This is simply to demonstrate that in a Certifai context, model type does not make a difference and allows us to compare any selection of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "champ_run, champion_model = local_run.get_output()\n",
    "\n",
    "challenger_run, challenger_model = local_run.get_output(12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certifai Scan Setup\n",
    "\n",
    "The below sections take us through the scan setup. We start with library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from certifai.scanner.builder import (CertifaiScanBuilder, CertifaiPredictorWrapper, CertifaiModel, CertifaiModelMetric,\n",
    "                                      CertifaiDataset, CertifaiGroupingFeature, CertifaiDatasetSource,\n",
    "                                      CertifaiPredictionTask, CertifaiTaskOutcomes, CertifaiOutcomeValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task definition\n",
    "\n",
    "This is the core of what we will evaluate. Specifically, we will be running:\n",
    "\n",
    "- Regression type task, as opposed to classification. The task is defined as `CertifaiTaskOutcomes.regression`\n",
    "- The favorable outcome is a reduction of the outcome variable, ie we consider favorable for the Taxi Ride to cost less rather than more `increased_favorable=False`\n",
    "- significant change is about 50% of the empirical standard deviation. `change_std_deviation=0.5`. In this case, the empirical standard deviation is about $9.6, hence we consider significant change to be about $5. \n",
    "\n",
    "\n",
    "We start defining a scan by assigning it this prediction task. In the remaining steps we will add additional characteristics of the scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task = CertifaiPredictionTask(CertifaiTaskOutcomes.regression(\n",
    "        increased_favorable=False,\n",
    "        change_std_deviation=0.5),\n",
    "    prediction_description='Predict taxi fare based on features of the trip')\n",
    "\n",
    "scan = CertifaiScanBuilder.create('test_user_case',\n",
    "                                  prediction_task=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Models & Data to the Scan\n",
    "\n",
    "We add the selected models to the scan. Here, we use a PredictorWrapper\n",
    "\n",
    "We also add the dataset by specifying its location. Here this is done via the `all_data_file` value we set earlier when preparing the data for training. Notice that this is simply a pointer to the location of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model up for use by Certifai as a local model\n",
    "champion_model_proxy = CertifaiPredictorWrapper(champion_model, encoder=encoder)\n",
    "challenger_model_proxy = CertifaiPredictorWrapper(challenger_model, encoder=encoder)\n",
    "\n",
    "# Add our local models\n",
    "first_model = CertifaiModel('champion',\n",
    "                            local_predictor=champion_model_proxy)\n",
    "scan.add_model(first_model)\n",
    "\n",
    "second_model = CertifaiModel('challenger',\n",
    "                            local_predictor=challenger_model_proxy)\n",
    "scan.add_model(second_model)\n",
    "\n",
    "# Add the eval dataset\n",
    "eval_dataset = CertifaiDataset('evaluation',\n",
    "                               CertifaiDatasetSource.csv(all_data_file))\n",
    "scan.add_dataset(eval_dataset)\n",
    "eval_dataset = CertifaiDataset('explanation',\n",
    "                               CertifaiDatasetSource.csv(sample_data_file))\n",
    "scan.add_dataset(eval_dataset)\n",
    "\n",
    "scan.evaluation_dataset_id = 'evaluation'\n",
    "# For this analysis we'll generate explanations for the entire dataset so we have a good number\n",
    "# on which to base statistical measures\n",
    "scan.explanation_dataset_id = 'explanation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Evaluations\n",
    "\n",
    "Adding evaluations is now very simple, one can just list the ones needed and those will be run by the scan and included in the result object.\n",
    "\n",
    "Notice that for 'explanation' and 'robustness', simply adding evaluation is sufficient to have the report run. For fairness, we also need to specify the actual sensitive feature we want to assess 'fairness' for. In this case: `passengers` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup an evaluation for explanation on the above dataset using the model\n",
    "scan.add_evaluation_type('explanation')\n",
    "scan.add_evaluation_type('robustness')\n",
    "scan.add_evaluation_type('fairness')\n",
    "\n",
    "scan.add_fairness_grouping_feature(CertifaiGroupingFeature('passengers'))\n",
    "\n",
    "\n",
    "# Run the scan.\n",
    "# By default this will write the results into individual report files (one per model and evaluation\n",
    "# type) in the 'reports' directory relative to the Jupyter root.  This may be disabled by specifying\n",
    "# `write_reports=False` as below\n",
    "# The result is a dictionary of dictionaries of reports.  The top level dict key is the evaluation type\n",
    "# and the second level key is model id.\n",
    "# Reports saved as JSON (which `write_reports=True` will do) may be visualized in the console app\n",
    "result = scan.run(write_reports=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Certifai Scan Results\n",
    "\n",
    "The results of the scan are stored in an extensive result variable. Here we would like to just preview the high-level outcomes.\n",
    "\n",
    "## Explanations\n",
    "We start with model level explanations to gauge what are the main drivers of taxi fares.\n",
    "\n",
    "What we will do here is essentially aggregate the number of times a certain variable has been used for generating a counterfactual, thus assessing its overall importance to the model\n",
    "\n",
    "The outcome is not surprising - by far the most important variable is distance traveled. Second in only a third of the first is pickup hour, which also makes sense as in some cases late evening and early morning hours are charged differently. Number of passengers  as well as the actual vendor used seem to play the least role in determining the fare. All of this is good as it validates our expectations on how taxi fares work and the sensibility of the models we have in place.\n",
    "\n",
    "Notice, that crucially, this level of detail is rarely available from simply training a model in itself or interpretability might be lost due to latent and/or dummy features generated along the way of making the model work. This analysis, however, is crucial for our understanding, confidence and ultimately trust in the behavior of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from tutorial_utils import get_feature_frequency, plot_histogram,plot_fairness_burden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot a histogram of frequency of occurrence of changes to each feature in counterfactuals\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[15,6])\n",
    "fig.suptitle('Feature occurrence frequency by model', fontsize=20)\n",
    "\n",
    "plot_histogram(ax1, 'champion', result)\n",
    "plot_histogram(ax2, 'challenger', result)\n",
    "\n",
    "# Put them both on the same scale\n",
    "ylim = max(ax1.get_ylim()[1], ax2.get_ylim()[1])\n",
    "ax1.set_ylim(top=ylim)\n",
    "ax2.set_ylim(top=ylim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness to different sized groups\n",
    "\n",
    "Finally, we set out here to determine the fairness of Taxi fares between differently sized groups of riders\n",
    "\n",
    "Looking at the results it seems that the burden on all groups is largely the same with groups of 4 passengers seemingly receiving a slightly higher burden than others. However, those also come with a much wider confidence range which, when taken into account, makes them not significantly different from other groups. Additionally, looking at group counts, groups of 4 are also far less present in the sample - only 41 examples out of ca 6250.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from certifai.scanner.report_utils import scores, construct_scores_dataframe\n",
    "\n",
    "\n",
    "df_rslt=construct_scores_dataframe(scores('fairness', result, max_depth=1))\n",
    "display(df_rslt)\n",
    "\n",
    "group_categories=[f\"({i}.0)\" for i in range(1,6)]\n",
    "group_xlabels=['single']+[f'{i} passengers' for i in range(2,6)]\n",
    "\n",
    "plot_fairness_burden(df_rslt,group_categories,group_xlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging Results\n",
    "\n",
    "Finally, using the workspace we already set up and assign a dedicated experiment for these runs. Here, we can log some key metrics for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment = Experiment(ws, \"certifai-rslt\")\n",
    "\n",
    "run = experiment.start_logging()\n",
    "run.log('Fairness-Champion',value=result['fairness']['champion']['fairness']['score'])\n",
    "run.log('Fairness-Challenger',value=result['fairness']['challenger']['fairness']['score'])\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
